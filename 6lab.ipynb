{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6a8c90c3-cb93-4da7-88e3-7b5a6bdd085b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data split into train/valid/test successfully.\n",
      "Train classes: ['accordion', 'airplanes', 'anchor']\n",
      "Valid classes: ['accordion', 'airplanes', 'anchor']\n",
      "Test classes: ['accordion', 'airplanes', 'anchor']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "# 1️⃣ Set paths\n",
    "# original_dir: Folder containing all class subfolders (e.g., accordion, airplanes)\n",
    "original_dir = \"dataset_101/3_ObjectCategories\"\n",
    "\n",
    "# base_dir: The folder where train/valid/test folders will be created\n",
    "base_dir = \"dataset_split\"\n",
    "\n",
    "# Split ratios for train, validation, and test sets\n",
    "split_ratios = {'train': 0.5, 'valid': 0.25, 'test': 0.25}\n",
    "\n",
    "# 2️⃣ Create train/valid/test folders\n",
    "# os.makedirs ensures the folder exists; exist_ok=True prevents error if folder already exists\n",
    "for split in split_ratios.keys():\n",
    "    os.makedirs(os.path.join(base_dir, split), exist_ok=True)\n",
    "\n",
    "# 3️⃣ Split images into train, validation, and test sets\n",
    "for category in os.listdir(original_dir):\n",
    "    class_dir = os.path.join(original_dir, category)  # Full path to class folder\n",
    "    if not os.path.isdir(class_dir):  # Skip if not a directory\n",
    "        continue\n",
    "\n",
    "    # List all image files in class folder (supports jpg, jpeg, png)\n",
    "    images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    # Shuffle images randomly to ensure random train/valid/test split\n",
    "    np.random.shuffle(images)\n",
    "\n",
    "    # Calculate number of images for each split\n",
    "    n_total = len(images)\n",
    "    n_train = int(n_total * split_ratios['train'])\n",
    "    n_valid = int(n_total * split_ratios['valid'])\n",
    "\n",
    "    # Create dictionary of split images\n",
    "    split_images = {\n",
    "        'train': images[:n_train],                        # First part for training\n",
    "        'valid': images[n_train:n_train+n_valid],         # Next part for validation\n",
    "        'test': images[n_train+n_valid:]                  # Remaining part for testing\n",
    "    }\n",
    "\n",
    "    # Copy images into their respective folders\n",
    "    for split, imgs in split_images.items():\n",
    "        split_class_dir = os.path.join(base_dir, split, category)  # Folder for this class & split\n",
    "        os.makedirs(split_class_dir, exist_ok=True)               # Ensure folder exists\n",
    "        for img in imgs:\n",
    "            shutil.copy(os.path.join(class_dir, img), os.path.join(split_class_dir, img))  # Copy image\n",
    "\n",
    "print(\"✅ Data split into train/valid/test successfully.\")\n",
    "\n",
    "# 4️⃣ Optional: Check which classes exist in each split\n",
    "print(\"Train classes:\", os.listdir(os.path.join(base_dir, 'train')))\n",
    "print(\"Valid classes:\", os.listdir(os.path.join(base_dir, 'valid')))\n",
    "print(\"Test classes:\", os.listdir(os.path.join(base_dir, 'test')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "10f37efc-cb9b-4876-a522-ae6aa682d7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset safely split into train/valid/test.\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "original_dir = \"dataset_101/3_ObjectCategories\"\n",
    "base_dir = \"dataset_101\"\n",
    "splits = ['train', 'valid', 'test']\n",
    "\n",
    "# Create directories\n",
    "for split in splits:\n",
    "    os.makedirs(os.path.join(base_dir, split), exist_ok=True)\n",
    "\n",
    "# Split images for each class safely\n",
    "for category in os.listdir(original_dir):\n",
    "    class_dir = os.path.join(original_dir, category)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    images = os.listdir(class_dir)\n",
    "    \n",
    "    # If only 1 image, put it in train\n",
    "    if len(images) == 1:\n",
    "        train_imgs = images\n",
    "        valid_imgs, test_imgs = [], []\n",
    "    elif len(images) == 2:\n",
    "        # 1 for train, 1 for test, no validation\n",
    "        train_imgs = [images[0]]\n",
    "        valid_imgs = []\n",
    "        test_imgs = [images[1]]\n",
    "    else:\n",
    "        # Normal split for 3+ images\n",
    "        train_imgs, temp_imgs = train_test_split(images, test_size=0.5, random_state=42)\n",
    "        if len(temp_imgs) == 1:\n",
    "            valid_imgs = [temp_imgs[0]]\n",
    "            test_imgs = []\n",
    "        else:\n",
    "            valid_imgs, test_imgs = train_test_split(temp_imgs, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Copy images to respective folders\n",
    "    for split_name, split_images in zip(splits, [train_imgs, valid_imgs, test_imgs]):\n",
    "        split_class_dir = os.path.join(base_dir, split_name, category)\n",
    "        os.makedirs(split_class_dir, exist_ok=True)\n",
    "        for img in split_images:\n",
    "            shutil.copy(os.path.join(class_dir, img), os.path.join(split_class_dir, img))\n",
    "\n",
    "print(\"✅ Dataset safely split into train/valid/test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1272cb21-64e2-4bf2-9e0f-fedc76e0fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Import required libraries\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Define directories\n",
    "# -----------------------------\n",
    "original_dir = \"dataset_101/101_ObjectCategories\"  # Your extracted folder\n",
    "base_dir = \"dataset_101_split\"  # Folder to hold train/valid/test\n",
    "\n",
    "splits = ['train', 'valid', 'test']\n",
    "for split in splits:\n",
    "    os.makedirs(os.path.join(base_dir, split), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6e99dbac-4391-43f6-87e4-4fdc1f617d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dir = \"dataset_101/3_ObjectCategories\"  # Correct folder with all classes\n",
    "base_dir = \"dataset_101_split\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fbd5f77e-22d8-45fa-b126-b6391c9c2129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset split completed successfully (handles small classes).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "original_dir = \"dataset_101/3_ObjectCategories\"  # folder containing all classes\n",
    "base_dir = \"dataset_101_split\"\n",
    "splits = ['train', 'valid', 'test']\n",
    "\n",
    "import shutil\n",
    "\n",
    "extra = '3_ObjectCategories'  # folder to remove\n",
    "shutil.rmtree(os.path.join(train_dir, extra), ignore_errors=True)\n",
    "shutil.rmtree(os.path.join(valid_dir, extra), ignore_errors=True)\n",
    "shutil.rmtree(os.path.join(test_dir, extra), ignore_errors=True)\n",
    "\n",
    "# Create split folders\n",
    "for split in splits:\n",
    "    os.makedirs(os.path.join(base_dir, split), exist_ok=True)\n",
    "\n",
    "# Loop through each class\n",
    "for category in os.listdir(original_dir):\n",
    "    class_dir = os.path.join(original_dir, category)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "\n",
    "    images = [img for img in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, img))]\n",
    "    n_images = len(images)\n",
    "    if n_images == 0:\n",
    "        continue\n",
    "    elif n_images == 1:\n",
    "        train_imgs = images\n",
    "        valid_imgs = []\n",
    "        test_imgs = []\n",
    "    elif n_images == 2:\n",
    "        train_imgs = [images[0]]\n",
    "        valid_imgs = [images[1]]\n",
    "        test_imgs = []\n",
    "    else:\n",
    "        # Normal case: split 50/25/25\n",
    "        train_imgs, temp_imgs = train_test_split(images, test_size=0.5, random_state=42)\n",
    "        valid_imgs, test_imgs = train_test_split(temp_imgs, test_size=0.5, random_state=42)\n",
    "\n",
    "    for split_name, split_images in zip(splits, [train_imgs, valid_imgs, test_imgs]):\n",
    "        split_class_dir = os.path.join(base_dir, split_name, category)\n",
    "        os.makedirs(split_class_dir, exist_ok=True)\n",
    "        for img in split_images:\n",
    "            src = os.path.join(class_dir, img)\n",
    "            dst = os.path.join(split_class_dir, img)\n",
    "            if os.path.isfile(src):\n",
    "                shutil.copy(src, dst)\n",
    "\n",
    "print(\"✅ Dataset split completed successfully (handles small classes).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "581a1ab5-7ca1-4fb1-8a72-a17b537be5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Import libraries\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 2️⃣ Set dataset paths\n",
    "train_dir = 'dataset_101_split/train'\n",
    "valid_dir = 'dataset_101_split/valid'\n",
    "# We'll skip test for now since images are very few\n",
    "\n",
    "# 3️⃣ Data augmentation for training, simple preprocessing for validation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e6c2b111-9c3c-4a28-a282-d05fdc4e43df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 images belonging to 3 classes.\n",
      "Found 3 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# 4️⃣ Create data generators\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=3,          # 1 image per class in batch\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "valid_gen = valid_datagen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=3,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ec20f366-7b4b-4144-8f43-1489dce6592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5️⃣ Load pretrained VGG16 model without top layer\n",
    "vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "# Freeze convolutional base\n",
    "vgg_base.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8829662f-6f2b-431c-823f-6741f92382bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6️⃣ Add custom classifier on top\n",
    "model = models.Sequential([\n",
    "    vgg_base,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(3, activation='softmax')  # 3 classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "43e53967-88f0-4ac4-8c77-86d374f0e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7️⃣ Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "01b69b78-9f93-4722-bc2c-d8037c22bfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - accuracy: 0.0000e+00 - loss: 1.3891 - val_accuracy: 0.3333 - val_loss: 2.3340\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.6667 - loss: 2.3191 - val_accuracy: 0.3333 - val_loss: 2.3882\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.3333 - loss: 2.6568 - val_accuracy: 0.3333 - val_loss: 2.1567\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.3333 - loss: 2.4568 - val_accuracy: 0.3333 - val_loss: 2.6616\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.3333 - loss: 3.0431 - val_accuracy: 0.3333 - val_loss: 2.5723\n"
     ]
    }
   ],
   "source": [
    "# 8️⃣ Train model\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=valid_gen,\n",
    "    epochs=5  # small number due to tiny dataset\n",
    ")\n",
    "\n",
    "# ✅ Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191cde15-5da8-4b99-a3d0-059456b45470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
