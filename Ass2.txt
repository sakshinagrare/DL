# Assignment No. 2: Feedforward Neural Network using Keras
# Dataset: MNIST Handwritten Digits
# ============================================================

# ----------------------------
# Step 1: Import Required Libraries
# ----------------------------
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelBinarizer

# TensorFlow / Keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

# ----------------------------
# Step 2: Load MNIST Dataset
# ----------------------------
# MNIST: 70,000 grayscale images (28x28 pixels) of handwritten digits (0–9)
(X_train, y_train), (X_test, y_test) = mnist.load_data()

print("Training data shape:", X_train.shape)
print("Testing data shape:", X_test.shape)

# Visualize one sample
plt.imshow(X_train[0], cmap='gray')
plt.title(f"Label: {y_train[0]}")
plt.axis('off')
plt.show()

# ----------------------------
# Step 3: Preprocess Data
# ----------------------------
# Flatten each 28x28 image into a 784-dimensional vector
X_train = X_train.reshape((X_train.shape[0], 28 * 28))
X_test = X_test.reshape((X_test.shape[0], 28 * 28))

# Normalize pixel values to [0, 1]
X_train = X_train.astype("float32") / 255.0
X_test = X_test.astype("float32") / 255.0

# One-hot encode the labels (0–9 → 10 output neurons)
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

print("Flattened train shape:", X_train.shape)
print("One-hot label shape:", y_train.shape)

# ----------------------------
# Step 4: Define Feedforward Neural Network Architecture
# ----------------------------
model = Sequential([
    Dense(256, input_shape=(784,), activation='sigmoid'),  # Hidden Layer 1
    Dense(128, activation='sigmoid'),                      # Hidden Layer 2
    Dense(10, activation='softmax')                        # Output Layer (10 classes)
])

# ----------------------------
# Step 5: Compile the Model
# ----------------------------
# Using Stochastic Gradient Descent (SGD) optimizer
model.compile(
    optimizer=SGD(learning_rate=0.01),
    loss='categorical_crossentropy',   # Suitable for classification
    metrics=['accuracy']
)

# Display model summary
model.summary()

# ----------------------------
# Step 6: Train the Model
# ----------------------------
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=10,            # Number of times data passes through the network
    batch_size=128,       # Number of samples per gradient update
    verbose=1
)

# ----------------------------
# Step 7: Evaluate the Model
# ----------------------------
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"✅ Test Accuracy: {test_acc * 100:.2f}%")
print(f"Test Loss: {test_loss:.4f}")

# ----------------------------
# Step 8: Plot Training Loss & Accuracy
# ----------------------------
plt.figure(figsize=(10,4))

# Accuracy Plot
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title("Model Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()

# Loss Plot
plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title("Model Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()

plt.tight_layout()
plt.show()

# ----------------------------
# Step 9: Classification Report
# ----------------------------
predictions = model.predict(X_test)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = np.argmax(y_test, axis=1)

print("Classification Report:")
print(classification_report(true_classes, predicted_classes, digits=4))